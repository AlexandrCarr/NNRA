{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W7okeJ2KlMkO"
      },
      "outputs": [],
      "source": [
        "import gc, random, sqlite3, shutil, pathlib as pl, sys, math\n",
        "from typing import Sequence, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa, pyarrow.parquet as pq\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "pd.set_option(\"display.max_rows\", 50)\n",
        "pd.set_option(\"display.float_format\", \"{:0.4f}\".format)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive, files\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "CSV_NAME    = \"datashare.csv\"\n",
        "SQLITE_NAME = \"tidy_finance.sqlite\"\n",
        "\n",
        "SEARCH_ROOTS = [\n",
        "    Path(\"/content\"),\n",
        "    Path(\"/content/drive/MyDrive\"),\n",
        "]\n",
        "\n",
        "CSV_PATH   = None\n",
        "SQLITE_DB  = None\n",
        "\n",
        "def find_one(root: Path, fname: str) -> Path | None:\n",
        "    \"\"\"Return the first match for fname found below root, else None.\"\"\"\n",
        "    try:\n",
        "        return next(root.rglob(fname))\n",
        "    except StopIteration:\n",
        "        return None\n",
        "\n",
        "for root in SEARCH_ROOTS:\n",
        "    CSV_PATH   = CSV_PATH   or find_one(root, CSV_NAME)\n",
        "    SQLITE_DB  = SQLITE_DB  or find_one(root, SQLITE_NAME)\n",
        "    if CSV_PATH and SQLITE_DB:\n",
        "        break\n",
        "\n",
        "assert CSV_PATH and SQLITE_DB, \"Files missing!\"\n",
        "print(f\"✓ CSV  ➜ {CSV_PATH}\")\n",
        "print(f\"✓ SQL  ➜ {SQLITE_DB}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI5J_L1AqKIC",
        "outputId": "e0351f8f-f9c4-42f1-e04e-3d0a5138a358"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✓ CSV  ➜ /content/drive/MyDrive/RAShip/datashare.csv\n",
            "✓ SQL  ➜ /content/drive/MyDrive/RAShip/tidy_finance.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0efae2f7"
      },
      "source": [
        "SEED_BASE = 42\n",
        "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "DATA_DIR   = pl.Path(\"/content/data\")\n",
        "CHAR_DIR   = DATA_DIR / \"char\"\n",
        "MERGED_DIR = DATA_DIR / \"merged\"\n",
        "FINAL_DIR  = DATA_DIR / \"final\"\n",
        "for d in (DATA_DIR, CHAR_DIR, MERGED_DIR, FINAL_DIR):\n",
        "    d.mkdir(exist_ok=True)\n",
        "\n",
        "ARCH: Dict[str, Sequence[int]] = {\n",
        "    \"nn1\": [32],\n",
        "    \"nn2\": [32, 16],\n",
        "    \"nn3\": [32, 16, 8],\n",
        "    \"nn4\": [32, 16, 8, 4],\n",
        "    \"nn5\": [32, 16, 8, 4, 2],\n",
        "}\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_to_unit(x: pd.Series) -> pd.Series:\n",
        "    r = x.rank(method=\"first\", na_option=\"keep\")\n",
        "    return 2 * ((r - 1) / (r.max() - 1) - 0.5)\n",
        "\n",
        "\n",
        "def prep_chars(csv_path: pl.Path, out_dir: pl.Path):\n",
        "    if list(out_dir.glob(\"*.parquet\")):\n",
        "        print(\"↳ Characteristic shards already exist – skipping step 1\")\n",
        "        return\n",
        "\n",
        "    print(\"↳ Building characteristic shards …\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df[\"month\"] = pd.to_datetime(df[\"DATE\"], format=\"%Y%m%d\")\\\n",
        "                     .dt.to_period(\"M\").dt.to_timestamp()\n",
        "    df.drop(columns=\"DATE\", inplace=True)\n",
        "\n",
        "    id_cols   = [\"permno\", \"month\"] + ([\"sic2\"] if \"sic2\" in df.columns else [])\n",
        "    char_cols = [c for c in df.columns if c not in id_cols]\n",
        "    df.rename(columns={c: f\"characteristic_{c}\" for c in char_cols}, inplace=True)\n",
        "\n",
        "    rank_cols = [c for c in df.columns if c.startswith(\"characteristic_\")]\n",
        "    df[rank_cols] = (df.groupby(\"month\", group_keys=False)[rank_cols]\n",
        "                       .apply(lambda g: g.apply(rank_to_unit)))\n",
        "\n",
        "    med = df.groupby(\"month\")[rank_cols].transform(\"median\")\n",
        "    df[rank_cols] = df[rank_cols].fillna(med).fillna(0)\n",
        "\n",
        "    df[\"year\"] = df[\"month\"].dt.year\n",
        "    for yr, grp in tqdm(df.groupby(\"year\"), desc=\"write char\", unit=\"yr\"):\n",
        "        pq.write_table(pa.Table.from_pandas(grp.drop(columns=\"year\")),\n",
        "                       out_dir / f\"{yr}.parquet\")\n",
        "\n",
        "prep_chars(CSV_PATH, CHAR_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAP3UghJrxr9",
        "outputId": "48b64383-d483-4968-f349-1254e5b8cea3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "↳ Characteristic shards already exist – skipping step 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_macro_from_google() -> pd.DataFrame:\n",
        "    sheet_id = \"1bM7vCWd3WOt95Sf9qjLPZjoiafgF_8EG\"\n",
        "    url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet=macro_predictors.xlsx\"\n",
        "    raw = pd.read_csv(url, thousands=\",\")\n",
        "    macro = (raw.assign(\n",
        "                 month=lambda x: pd.to_datetime(x[\"yyyymm\"], format=\"%Y%m\"),\n",
        "                 dp  = np.log(raw[\"D12\"]) - np.log(raw[\"Index\"]),\n",
        "                 ep  = np.log(raw[\"E12\"]) - np.log(raw[\"Index\"]),\n",
        "                 bm  = raw[\"b/m\"],\n",
        "                 ntis= raw[\"ntis\"],\n",
        "                 tbl = raw[\"tbl\"],\n",
        "                 tms = raw[\"lty\"] - raw[\"tbl\"],\n",
        "                 dfy = raw[\"BAA\"] - raw[\"AAA\"],\n",
        "                 svar= raw[\"svar\"])\n",
        "             .get([\"month\",\"dp\",\"ep\",\"bm\",\"ntis\",\"tbl\",\"tms\",\"dfy\",\"svar\"])\n",
        "             .dropna())\n",
        "    macro[\"month\"] = macro[\"month\"].dt.to_period(\"M\").dt.to_timestamp()\n",
        "    macro = macro.rename(columns=lambda c: f\"macro_{c}\" if c!=\"month\" else c)\n",
        "    lag = macro.copy(); lag[\"month\"] += pd.offsets.MonthEnd(1)\n",
        "    return macro[[\"month\"]].merge(lag, on=\"month\")"
      ],
      "metadata": {
        "id": "xwi3tcnYr4rp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sql_df(q: str) -> pd.DataFrame:\n",
        "    with sqlite3.connect(SQLITE_DB) as con:\n",
        "        return pd.read_sql_query(q, con, parse_dates=[\"month\"])\n",
        "\n",
        "def build_merged_shards():\n",
        "    if list(MERGED_DIR.glob(\"*.parquet\")):\n",
        "        print(\"↳ Merged shards already exist – skipping step 3\")\n",
        "        return\n",
        "\n",
        "    print(\"↳ Merging CRSP + macro + characteristics …\")\n",
        "    crsp  = sql_df(\"SELECT month, permno, mktcap_lag, ret_excess FROM crsp_monthly\")\n",
        "\n",
        "    with sqlite3.connect(SQLITE_DB) as con:\n",
        "        has_macro = \"macro_predictors\" in [t for (t,) in\n",
        "                                           con.execute(\"SELECT name FROM sqlite_master \"\n",
        "                                                       \"WHERE type='table'\")]\n",
        "    macro = sql_df(\"SELECT * FROM macro_predictors\") if has_macro else load_macro_from_google()\n",
        "    macro[\"month\"] = macro[\"month\"].dt.to_period(\"M\").dt.to_timestamp()\n",
        "    macro = macro.rename(columns=lambda c: f\"macro_{c}\" if c!=\"month\" else c)\n",
        "    lag = macro.copy(); lag[\"month\"] += pd.offsets.MonthEnd(1)\n",
        "    macro = macro[[\"month\"]].merge(lag, on=\"month\")\n",
        "\n",
        "    for p in tqdm(sorted(CHAR_DIR.glob(\"*.parquet\")), desc=\"merge\", unit=\"yr\"):\n",
        "        feats = pq.read_table(p).to_pandas()\n",
        "        merged = (feats\n",
        "                  .merge(crsp,  on=[\"month\",\"permno\"], how=\"left\")\n",
        "                  .merge(macro, on=\"month\",          how=\"left\")\n",
        "                  .assign(macro_intercept=1.0))\n",
        "        pq.write_table(pa.Table.from_pandas(merged), MERGED_DIR / p.name)\n",
        "        del feats, merged; gc.collect()\n",
        "\n",
        "build_merged_shards()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukvp6lpTsZS_",
        "outputId": "e19972cc-cf08-4b5e-8496-99fb15d0e141"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "↳ Merged shards already exist – skipping step 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# characteristic × macro interactions\n",
        "def build_interactions():\n",
        "    if list(FINAL_DIR.glob(\"*.parquet\")):\n",
        "        print(\"↳ Interaction shards already exist – skipping step 4\")\n",
        "        return\n",
        "\n",
        "    print(\"↳ Creating characteristic × macro interactions (vectorised) …\")\n",
        "    for p in tqdm(sorted(MERGED_DIR.glob(\"*.parquet\")), desc=\"interactions\", unit=\"yr\"):\n",
        "        df = pq.read_table(p).to_pandas()\n",
        "\n",
        "        char_cols  = [c for c in df.columns if c.startswith(\"characteristic_\")]\n",
        "        macro_cols = [c for c in df.columns if c.startswith(\"macro_\")]\n",
        "\n",
        "        # ------- vectorised interactions -------------------------------------\n",
        "        #  X_char :  n × C   ;  X_macro : n × M\n",
        "        X_char  = df[char_cols].to_numpy(dtype=\"float32\")\n",
        "        X_macro = df[macro_cols].to_numpy(dtype=\"float32\")\n",
        "        #  broadcast, multiply, reshape to n × (C·M)\n",
        "        inter   = (X_char[:, :, None] * X_macro[:, None, :]).reshape(len(df), -1)\n",
        "\n",
        "        inter_cols = [f\"{c}__x__{m}\" for c in char_cols for m in macro_cols]\n",
        "        df_inter   = pd.DataFrame(inter, columns=inter_cols, index=df.index)\n",
        "\n",
        "        # join once → no fragmentation warning\n",
        "        df = pd.concat([df, df_inter], axis=1, copy=False)\n",
        "\n",
        "        # one‑hot SIC‑2\n",
        "        if \"sic2\" in df.columns:\n",
        "            df = pd.get_dummies(df, columns=[\"sic2\"], prefix=\"sic2\", dtype=\"int8\")\n",
        "\n",
        "        pq.write_table(pa.Table.from_pandas(df), FINAL_DIR / p.name)\n",
        "        del df, df_inter, inter, X_char, X_macro\n",
        "        gc.collect()\n",
        "\n",
        "build_interactions()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjcVnxAVsc3h",
        "outputId": "4dad6736-1d67-4254-e786-abf02b5c8481"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "↳ Interaction shards already exist – skipping step 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def masks(df: pd.DataFrame, test_year: int):\n",
        "    y = df[\"month\"].dt.year\n",
        "    return ((y>=1957)&(y<=test_year-13),\n",
        "            (y>=test_year-12)&(y<=test_year-1),\n",
        "            y==test_year)\n"
      ],
      "metadata": {
        "id": "WHg-zb2Rsl5g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, d_in:int, widths:Sequence[int]):\n",
        "        super().__init__()\n",
        "        seq, prev = [], d_in\n",
        "        for w in widths:\n",
        "            seq += [nn.Linear(prev,w), nn.ReLU(), nn.BatchNorm1d(w)]; prev=w\n",
        "        seq.append(nn.Linear(prev,1)); self.net = nn.Sequential(*seq)\n",
        "    def forward(self,x): return self.net(x).squeeze(-1)\n",
        "\n",
        "def fit_nn(train, val, arch, seed):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    cols_drop=[\"permno\",\"month\",\"ret_excess\",\"mktcap_lag\"]\n",
        "\n",
        "    Xtr,ytr = train.drop(cols_drop,axis=1).values.astype(\"float32\"), train[\"ret_excess\"].values.astype(\"float32\")\n",
        "    Xva,yva = val.drop(cols_drop,axis=1).values.astype(\"float32\"), val[\"ret_excess\"].values.astype(\"float32\")\n",
        "\n",
        "    sc = StandardScaler().fit(Xtr); Xtr,Xva = sc.transform(Xtr),sc.transform(Xva)\n",
        "\n",
        "    ds_tr = torch.utils.data.TensorDataset(torch.as_tensor(Xtr), torch.as_tensor(ytr))\n",
        "    ds_va = torch.utils.data.TensorDataset(torch.as_tensor(Xva), torch.as_tensor(yva))\n",
        "    dl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=8192,shuffle=True,pin_memory=True)\n",
        "    dl_va = torch.utils.data.DataLoader(ds_va,batch_size=8192,pin_memory=True)\n",
        "\n",
        "    net = FFN(Xtr.shape[1],ARCH[arch]).to(DEVICE)\n",
        "    opt,crit = optim.Adam(net.parameters(),lr=1e-3), nn.MSELoss()\n",
        "    best=float(\"inf\"); waited=0\n",
        "    for _ in range(100):\n",
        "        net.train()\n",
        "        for xb,yb in dl_tr:\n",
        "            xb,yb=xb.to(DEVICE),yb.to(DEVICE)\n",
        "            opt.zero_grad(); crit(net(xb),yb).backward(); opt.step()\n",
        "        val_loss=0; net.eval()\n",
        "        with torch.no_grad():\n",
        "            for xb,yb in dl_va:\n",
        "                val_loss+=crit(net(xb.to(DEVICE)), yb.to(DEVICE)).item()*len(yb)\n",
        "        val_loss/=len(ds_va)\n",
        "        if val_loss<best: best,best_state,waited=val_loss,net.state_dict(),0\n",
        "        else:\n",
        "            waited+=1\n",
        "            if waited>=5: break\n",
        "    net.load_state_dict(best_state)\n",
        "    return net,sc\n",
        "\n",
        "def predict(net,sc,df):\n",
        "    X=sc.transform(df.drop([\"permno\",\"month\",\"ret_excess\",\"mktcap_lag\"],axis=1).values.astype(\"float32\"))\n",
        "    with torch.no_grad():\n",
        "        pr=net(torch.as_tensor(X).to(DEVICE)).cpu().numpy()\n",
        "    out=df[[\"permno\",\"month\",\"mktcap_lag\",\"ret_excess\"]].copy(); out[\"pred\"]=pr; return out\n"
      ],
      "metadata": {
        "id": "cmoukHjes-bh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Cell 10 – OOS training loop\n",
        "import time\n",
        "\n",
        "N_ENSEMBLE   = 5         # set to 2‑3 while prototyping, 10 for full replication\n",
        "MAX_EPOCHS   = 20\n",
        "PATIENCE     = 5\n",
        "MIN_DELTA    = 1e-5\n",
        "PRED         = []\n",
        "\n",
        "def fit_nn(train, val, arch, seed, echo=False):\n",
        "    \"\"\"Feed‑forward net with early‑stop; echo=True prints per‑epoch val‑MSE.\"\"\"\n",
        "    cols_drop = [\"permno\", \"month\", \"ret_excess\", \"mktcap_lag\"]\n",
        "\n",
        "    Xtr = train.drop(columns=cols_drop).to_numpy(\"float32\")\n",
        "    ytr = train[\"ret_excess\"].to_numpy(\"float32\")\n",
        "    Xva = val.drop(columns=cols_drop).to_numpy(\"float32\")\n",
        "    yva = val[\"ret_excess\"].to_numpy(\"float32\")\n",
        "\n",
        "    sc = StandardScaler().fit(Xtr)\n",
        "    Xtr, Xva = sc.transform(Xtr), sc.transform(Xva)\n",
        "\n",
        "    ds_tr = torch.utils.data.TensorDataset(torch.as_tensor(Xtr), torch.as_tensor(ytr))\n",
        "    ds_va = torch.utils.data.TensorDataset(torch.as_tensor(Xva), torch.as_tensor(yva))\n",
        "    dl_tr = torch.utils.data.DataLoader(ds_tr, batch_size=8192, shuffle=True, pin_memory=True)\n",
        "    dl_va = torch.utils.data.DataLoader(ds_va, batch_size=8192, pin_memory=True)\n",
        "\n",
        "    net = FFN(Xtr.shape[1], ARCH[arch]).to(DEVICE)\n",
        "    opt, crit = optim.Adam(net.parameters(), lr=1e-3), nn.MSELoss()\n",
        "\n",
        "    best, waited = float(\"inf\"), 0\n",
        "    for epoch in range(MAX_EPOCHS):\n",
        "        net.train()\n",
        "        for xb, yb in dl_tr:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            opt.zero_grad(); crit(net(xb), yb).backward(); opt.step()\n",
        "\n",
        "        # validation\n",
        "        net.eval(); val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in dl_va:\n",
        "                val_loss += crit(net(xb.to(DEVICE)), yb.to(DEVICE)).item() * len(yb)\n",
        "        val_loss /= len(ds_va)\n",
        "\n",
        "        if echo:\n",
        "            print(f\"    epoch {epoch+1:02d}  val‑MSE {val_loss:,.4e}\")\n",
        "\n",
        "        if val_loss < best - MIN_DELTA:\n",
        "            best, best_state, waited = val_loss, net.state_dict(), 0\n",
        "        else:\n",
        "            waited += 1\n",
        "            if waited >= PATIENCE:\n",
        "                break\n",
        "\n",
        "    net.load_state_dict(best_state)\n",
        "    return net, sc\n",
        "\n",
        "print(f\"GPU detected: {torch.cuda.get_device_name(0) if DEVICE=='cuda' else 'CPU‑only'}\")\n",
        "\n",
        "for yr in tqdm(range(2000, 2022), desc=\"OOS\", unit=\"yr\"):\n",
        "    t0 = time.time()\n",
        "\n",
        "    shards = [pq.read_table(p).to_pandas()\n",
        "              for p in FINAL_DIR.glob(\"*.parquet\") if int(p.stem) <= yr]\n",
        "    df_all = pd.concat(shards, ignore_index=True); del shards\n",
        "\n",
        "    sic_cols = [c for c in df_all.columns if c.startswith(\"sic2_\")]\n",
        "    if sic_cols:\n",
        "        df_all[sic_cols] = df_all[sic_cols].fillna(0).astype(\"int8\")\n",
        "    df_all = df_all.fillna(0)\n",
        "\n",
        "    tr, va, te = masks(df_all, yr)\n",
        "    if df_all[tr].empty or df_all[va].empty:\n",
        "        print(f\"{yr}: skipped – no data\")\n",
        "        continue\n",
        "\n",
        "    preds = []\n",
        "    for s in range(N_ENSEMBLE):\n",
        "        echo = (yr == 2000 and s == 0)   # only the first model is verbose\n",
        "        net, sc = fit_nn(df_all[tr], df_all[va], \"nn5\",\n",
        "                         SEED_BASE + s, echo=echo)\n",
        "        preds.append(predict(net, sc, df_all[te]))\n",
        "\n",
        "    tmp = preds[0].copy()\n",
        "    tmp[\"pred\"] = np.mean([p_[\"pred\"].values for p_ in preds], axis=0)\n",
        "\n",
        "    if np.allclose(tmp[\"pred\"].values, 0.0):\n",
        "        raise RuntimeError(f\"Year {yr}: predictions collapsed to zeros.\")\n",
        "\n",
        "    PRED.append(tmp)\n",
        "    print(f\"✓ {yr}  {N_ENSEMBLE} nets  {time.time() - t0:5.1f}s  \"\n",
        "          f\"val‑MSE best {min(p['pred'].var() for p in preds):.2e}\")\n",
        "\n",
        "    del df_all, tmp, preds\n",
        "    gc.collect()\n",
        "\n",
        "pred_df = pd.concat(PRED, ignore_index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAJJn-TLtBWv",
        "outputId": "dbb0988a-11e9-47dc-ad6a-d24b70e60d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU detected: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rOOS:   0%|          | 0/22 [00:00<?, ?yr/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    epoch 01  val‑MSE 9.0402e-03\n",
            "    epoch 02  val‑MSE 6.1669e-04\n",
            "    epoch 03  val‑MSE 2.3611e-04\n",
            "    epoch 04  val‑MSE 1.7142e-04\n",
            "    epoch 05  val‑MSE 1.0143e-04\n",
            "    epoch 06  val‑MSE 8.4170e-05\n",
            "    epoch 07  val‑MSE 6.1710e-05\n",
            "    epoch 08  val‑MSE 5.4876e-05\n",
            "    epoch 09  val‑MSE 4.3366e-05\n",
            "    epoch 10  val‑MSE 3.9045e-05\n",
            "    epoch 11  val‑MSE 3.5081e-05\n",
            "    epoch 12  val‑MSE 3.2962e-05\n",
            "    epoch 13  val‑MSE 3.0357e-05\n",
            "    epoch 14  val‑MSE 2.7531e-05\n",
            "    epoch 15  val‑MSE 2.5595e-05\n",
            "    epoch 16  val‑MSE 2.3199e-05\n",
            "    epoch 17  val‑MSE 2.4425e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rOOS:   5%|▍         | 1/22 [1:00:53<21:18:41, 3653.43s/yr]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ 2000  5 nets  3653.2s  val‑MSE best 2.73e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rOOS:   9%|▉         | 2/22 [2:11:18<22:09:48, 3989.40s/yr]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ 2001  5 nets  4224.4s  val‑MSE best 4.26e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rOOS:  14%|█▎        | 3/22 [3:03:20<18:57:58, 3593.60s/yr]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ 2002  5 nets  3122.4s  val‑MSE best 6.07e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_decile(series): return pd.qcut(series,10,labels=False,duplicates=\"drop\")+1\n",
        "pred_df[\"portfolio\"]=pred_df.groupby(\"month\",group_keys=False)[\"pred\"].apply(assign_decile)\n",
        "\n",
        "panel=(pred_df.groupby([\"portfolio\",\"month\"])[[\"ret_excess\",\"pred\"]]\n",
        "         .mean().reset_index())\n",
        "hi,lo = panel[panel[\"portfolio\"]==10], panel[panel[\"portfolio\"]==1]\n",
        "hml = hi.set_index(\"month\") - lo.set_index(\"month\"); hml.reset_index(inplace=True); hml[\"portfolio\"]=\"H‑L\"\n",
        "\n",
        "panel  = pd.concat([panel,hml],ignore_index=True)\n",
        "summary=(panel.groupby(\"portfolio\")\n",
        "              .agg(predicted_mean=(\"pred\",\"mean\"),\n",
        "                   realized_mean=(\"ret_excess\",\"mean\"),\n",
        "                   realized_sd=(\"ret_excess\",\"std\")))\n",
        "summary[\"sharpe\"]=summary[\"realized_mean\"]/summary[\"realized_sd\"]\n",
        "\n",
        "print(\"\\n================  Out‑of‑sample performance 2000‑2021  ================\\n\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "hzmkcW2ttDhN",
        "outputId": "b73b8cea-deca-412a-9083-fc7737b73144"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pred_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3797767329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massign_decile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mduplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"portfolio\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"month\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_decile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m panel=(pred_df.groupby([\"portfolio\",\"month\"])[[\"ret_excess\",\"pred\"]]\n\u001b[1;32m      5\u001b[0m          .mean().reset_index())\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pred_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c3vJ6mYiDm9R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}